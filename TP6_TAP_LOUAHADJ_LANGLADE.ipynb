{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Binôme :** LOUAHADJ Aniss et LANGLADE Alexandre"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TP N°6 MODELES DE LANGAGE STATISTIQUES\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Sauvegardez le fichier en mettant votre NOM dans le nom du notebook. Ce document servira de compte rendu de TP et devra être déposé sur moodle à la fin de la séance. \n",
    "\n",
    "Après avoir récupéré et lu le sujet, répondre à chacune des questions posées en : \n",
    "1) Récupérant la commande exécutée sous ubuntu ainsi que la trace associée \n",
    "2) Commentant le résultat et/ou en apportant des éléments de réponses dans la zone de texte dédiée\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "RAPPELS: les commandes à utiliser sont des lignes de commandes Unix (cf 1A). Les mécanismes de redirection sont donc utilisés pour (1) envoyer des données en entrée de la commande (<) (2) sauvegarder des données de sortie dans un fichier (>) (3) et  envoyer le résultat d’une commande à la commande suivante (|). Les [] indiquent que le contenu est optionnel et que la valeur indiquée est la valeur par défaut. Ne pas inscrire explicitement les [] dans les commandes ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### APPRENTISSAGE DES MODELES DE LANGAGE N-GRAMMES"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Génération de la liste de mots et calcul du nombre d’occurrences\n",
    "\n",
    "QUESTION 1 : En utilisant l’ensemble des fichiers dédiés à l’apprentissage (CORPUS_APP), utiliser l’outil text2wfreq pour générer la liste des mots présents dans ces fichiers ainsi que leur nombre d’occurrences (fichier .wfreq).\n",
    "./CMU-Cam_Toolkit_v2/bin/text2wfreq < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app.wfreq\n",
    "text2wfreq : Reading text from standard input...\n",
    "text2wfreq : Done.\n",
    "\n",
    "On a : \n",
    "\n",
    "A 1\n",
    "E 1\n",
    "J 2\n",
    "accompagn<E9>s 1\n",
    "U 4\n",
    "V 1\n",
    "X 1\n",
    "a 158\n",
    "c 1\n",
    "d 1\n",
    "g 1\n",
    "s 5\n",
    "y 27\n",
    "injections 1\n",
    "surtout 14\n",
    "petit 7\n",
    "amateurs 1\n",
    "promesse 1\n",
    "client 1\n",
    "privil<E9>gi<E9>e 1\n",
    "navire 2\n",
    "pr<E9>venir 1\n",
    "soixante-trois 1\n",
    "faudrait 1\n",
    "n'en 1\n",
    "vendredi 6\n",
    "confrontation 1\n",
    "\n",
    "\n",
    "Christrian 1\n",
    "agr<E9>ables 1\n",
    "terminer 2\n",
    "mets 1\n",
    "subite 1\n",
    "disparus 1\n",
    "musicales 1\n",
    "oeillet 1\n",
    "l'avez 2\n",
    "r<E9>gl<E9> 3\n",
    "assi<E9>g<E9> 1\n",
    "comportement 1\n",
    "Liban 3\n",
    "environnement 1\n",
    "ce_n'est_pas 7\n",
    "courageuse 1\n",
    "majoritaire 1\n",
    "combattu 1\n",
    "globale 1\n",
    "r<E8>gles 3\n",
    "remodeler 1\n",
    "doivent 5\n",
    "optimistes 1\n",
    "Gr<E9>gori 1\n",
    "d<E9>plaire 2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# text2wfreq \t[ -hash 1000000 ] [ -verbosity 2 ] < .text     > .wfreq\n",
    "# exécuter la commande dans un terminal, copier coller la commande et la trace et copier coller le début (10 premières # lignes commande head) et la fin du fichier généré (10 dernières lignes commande tail). Trop long sinon \n",
    "... "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 2 : A l’aide de la commande unix sort, trier la liste de façon à la classer les mots par nombres d’occurrences décroissants. Quels sont les mots les plus fréquents ? A quoi correspondent-ils et pourquoi ? "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Réponse : \n",
    "sort -nr -k2 CORPUS_TP/CORPUS_APP/app.txt.wfreq | less\n",
    "\n",
    "<S> 1277\n",
    "</S> 1277\n",
    "de 1073\n",
    "la 701\n",
    "le 611\n",
    "l' 500\n",
    "et 456\n",
    "les 454\n",
    "<E0> 444\n",
    "en 335\n",
    "un 289\n",
    "des 259\n",
    "du 242\n",
    "dans 241\n",
    "qui 208\n",
    "que 197\n",
    "est 187\n",
    "d' 187\n",
    "il 176\n",
    "une 168\n",
    "pour 163\n",
    "sur 162\n",
    "a 158\n",
    "\n",
    "\n",
    "Les 2 premiers mots correspondent aux débuts et aux fins des phrases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Génération du vocabulaire\n",
    "\n",
    "QUESTION 3 : Utiliser l’outil wfreq2vocab pour obtenir la liste des mots distincts correspondant au vocabulaire extrait de ce corpus d’apprentissage (fichier .vocab). Examinez cette liste.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# wfreq2vocab \t[ -top 20000 | -gt 10] [ -records 1000000 ] [ -verbosity 2]  < .wfreq     > .vocab\n",
    "# exécuter la commande dans un terminal, copier coller la commande et la trace et copier coller le début (10 premières # lignes commande head) et la fin du fichier généré (10 dernières lignes commande tail). Trop long sinon \n",
    "...\n",
    "./CMU-Cam_Toolkit_v2/bin/wfreq2vocab < CORPUS_TP/CORPUS_APP/app.wfreq > CORPUS_TP/CORPUS_APP/app.vocab\n",
    "wfreq2vocab : Will generate a vocabulary containing the most\n",
    "              frequent 20000 words. Reading wfreq stream from stdin...\n",
    "wfreq2vocab : Done.\n",
    "\n",
    "## Vocab generated by v2 of the CMU-Cambridge Statistcal\n",
    "## Language Modeling toolkit.\n",
    "##\n",
    "## Includes 4797 words ##\n",
    "(et)\n",
    "-ce\n",
    "-elle\n",
    "-elles\n",
    "-il\n",
    "-ils\n",
    "-le\n",
    "-l<E0>\n",
    "-moi\n",
    "-nous\n",
    "-on\n",
    "-t-elle\n",
    "-t-il\n",
    "-t-on\n",
    "-vous\n",
    "-y\n",
    "</S>\n",
    "<S>\n",
    "\n",
    "<E9>trang<E8>re\n",
    "<E9>trang<E8>res\n",
    "<E9>troites\n",
    "<E9>tudes\n",
    "<E9>tudier\n",
    "<E9>t<E9>\n",
    "<E9>vacu<E9>\n",
    "<E9>vadent\n",
    "<E9>vad<E9>es\n",
    "<E9>vad<E9>s\n",
    "<E9>valuation\n",
    "<E9>vasion\n",
    "<E9>vasions\n",
    "<E9>ventuellement\n",
    "<E9>videmment\n",
    "<E9>vidence\n",
    "<E9>vitant\n",
    "<E9>viter\n",
    "<E9>vit<E9>\n",
    "<E9>vocation\n",
    "<E9>voquant\n",
    "<E9>voquent\n",
    "<E9>voquer\n",
    "<E9>voqueront\n",
    "<E9>voqu<E9>\n",
    "<E9>voqu<E9>e\n",
    "<E9>v<E8>nement\n",
    "<EA>tes\n",
    "<EA>tre\n",
    "\n",
    "on voit que l'on a toujours des majuscules et des minuscules, et certains caractères ne s'affichent pas (accents,problème d'encodage...)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Liste des bigrammes et trigrammes et nombres d’occurrences associés\n",
    "\n",
    "QUESTION 4 : Utiliser l’outil text2wngram pour générer l’ensemble des paires de mots présentes dans le corpus d’apprentissage (fichier .wngram) et compter leur nombre d’occurrences. \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# text2wngram [ -n 3 ] [ -temp /usr/tmp/ ][ -chars n ] [ -words m ][ -gzip | -compress ] [ -verbosity 2 ] < .text     #  > .wngram\n",
    "# exécuter la commande dans un terminal, copier coller la commande et la trace et copier coller le début (10 premières # lignes commande head) et la fin du fichier généré (10 dernières lignes commande tail). Trop long sinon \n",
    "..."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./CMU-Cam_Toolkit_v2/bin/text2wngram -n 2 -temp .  < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app3.wngram\n",
    "\n",
    "text2wngram\n",
    "n = 2\n",
    "Number of words in buffer = 9090909\n",
    "Number of chars in buffer = 63636363\n",
    "Max number of files open at once = 20\n",
    "Temporary directory = ./\n",
    "Allocated 63636363 bytes to text buffer.\n",
    "Allocated 72727272 bytes to pointer array.\n",
    "Reading text into buffer...\n",
    "Reading text into the n-gram buffer...\n",
    "20,000 words processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Sorting pointer array...\n",
    "Writing out temporary file ./text2wngram.tmp.pc65l.7480.1...\n",
    "Merging temporary files...\n",
    "Merging temp files 1 through 1...\n",
    "text2wngram : Done."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Quels sont les bigrammes les plus fréquents ? A quoi correspondent-ils ? \n",
    "\n",
    "</S> <S> 1276 #fin phrase debut phrase\n",
    "de la 194\n",
    "de l' 129\n",
    "dans le 77\n",
    "<E0> l' 65 #un caractere non affiché (accents)\n",
    "<S> les 60\n",
    "<E0> la 60\n",
    "<S> le 59\n",
    "<S> et 53\n",
    "<S> la 52\n",
    "France Inter 51\n",
    "<S> il 44\n",
    "<S> c'est 42\n",
    "dans la 42\n",
    "<S> on 39\n",
    "il est 38\n",
    "<S> l' 37\n",
    "sur le 36\n",
    "<S> bonjour 34\n",
    "la guerre 31\n",
    "sept heures 30\n",
    "sur la 29\n",
    "<S> en 29\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 5 : Utiliser l’outil text2idngram pour générer une autre version de cette liste (fichier .idngram) et comparer le résultat avec le précédent. Ne pas oublier l’option (-write_ascii) pour générer le résultat sous forme ascii et non binaire plus difficile à lire mais plus efficace en terme de traitement …"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# text2idngram -vocab .vocab [ -buffer 100 ] [ -temp /usr/tmp/ ] [ -files 20 ] [ -gzip | -compress ] [ -n 3 ] \n",
    "# [ -write_ascii ] [ -fof_size 10 ] [ -verbosity 2 ]< .text > .idngram \n",
    "\n",
    "# exécuter la commande dans un terminal, copier coller la commande et la trace et copier coller le début (10 premières # lignes commande head) et la fin du fichier généré (10 dernières lignes commande tail). Trop long sinon \n",
    "...\n",
    "\n",
    "commande :  ./CMU-Cam_Toolkit_v2/bin/text2idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -temp . -n 2 -write_ascii < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app2.idngram\n",
    "\n",
    "Trace :\n",
    "\n",
    "text2idngram\n",
    "Vocab                  : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "N-gram buffer size     : 100\n",
    "Hash table size        : 200000\n",
    "Temp directory         : ./\n",
    "Max open files         : 20\n",
    "FOF size               : 10\n",
    "n                      : 2\n",
    "Initialising hash table...\n",
    "Reading vocabulary...\n",
    "Allocating memory for the n-gram buffer...\n",
    "Reading text into the n-gram buffer...\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Sorting n-grams...\n",
    "Writing sorted n-grams to temporary file ./text2idngram.temp.pc65l.7486.1\n",
    "Merging temporary files...\n",
    "\n",
    "2-grams occurring:\tN times\t\t> N times\tSug. -spec_num value\n",
    "      0\t\t\t      16340\t\t  16513\n",
    "      1\t\t\t\t  13291\t\t   3049\t\t   3089\n",
    "      2\t\t\t\t   1732\t\t   1317\t\t   1340\n",
    "      3\t\t\t\t    542\t\t    775\t\t    792\n",
    "      4\t\t\t\t    268\t\t    507\t\t    522\n",
    "      5\t\t\t\t    143\t\t    364\t\t    377\n",
    "      6\t\t\t\t     91\t\t    273\t\t    285\n",
    "      7\t\t\t\t     52\t\t    221\t\t    233\n",
    "      8\t\t\t\t     48\t\t    173\t\t    184\n",
    "      9\t\t\t\t     34\t\t    139\t\t    150\n",
    "     10\t\t\t\t     20\t\t    119\t\t    130\n",
    "text2idngram : Done.\n",
    "\n",
    "\n",
    "Premieres lignes :\n",
    "1 3428 1\n",
    "2 1050 1\n",
    "2 2758 1\n",
    "2 3670 4\n",
    "3 17 1\n",
    "3 2618 1\n",
    "3 3990 1\n",
    "3 4449 1\n",
    "4 1932 1\n",
    "4 4774 1\n",
    "5 17 1\n",
    "5 2051 1\n",
    "5 2652 1\n",
    "\n",
    "Dernieres lignes :\n",
    "4797 1630 1\n",
    "4797 1818 1\n",
    "4797 1958 1\n",
    "4797 1962 1\n",
    "4797 1999 1\n",
    "4797 2128 1\n",
    "4797 2179 1\n",
    "4797 2236 1\n",
    "4797 2243 1\n",
    "4797 2530 1\n",
    "4797 3172 1\n",
    "4797 3269 1\n",
    "4797 3574 1\n",
    "4797 3589 1\n",
    "4797 3780 1\n",
    "4797 4129 1\n",
    "4797 4416 1\n",
    "4797 4475 1\n",
    "4797 4478 1\n",
    "4797 4660 1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 6 : Examinez la trace affichée pendant l’exécution de la commande et commentez les éléments qui vous semblent significatifs.\n",
    "\n",
    "La trace indique :\n",
    "Il y a 16340 bigrammes que l'on peut construire à partir du vocabulaire qui apparaissent 0 fois dans le texte.\n",
    "Il y a 13291 bigrammes que l'on peut construire à partir du vocabulaire qui apparaissent 1 fois dans le texte.\n",
    "Etc...\n",
    "Ensuite, la colonne suivante indique la somme des apparitions des bigrammes qui apparaissent plus de fois que le nombre d'apparitions considéré..\n",
    "La dernière colonne est une suggestion de la colonne précédente.\n",
    "\n",
    "Les fichiers idgrams donnent le nombre d'apparitions des bigrammes constitués des mots id1 et id2, l'id étant l'identifiant d'un mot (un entier) appartenant au vocabulaire (c'est la meme chose que wngram mais avec les identifiants des mots plutot que les mots eux-memes).\n",
    "\n",
    "Relancer sans l’option -write_ascii pour obtenir une version binaire à utiliser par la suite (question 8).  \n",
    " Commande + trace\n",
    "\n",
    "Commande :\n",
    " ./CMU-Cam_Toolkit_v2/bin/text2idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -temp . -n 2 < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app2.idngram\n",
    "text2idngram\n",
    "Vocab                  : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "N-gram buffer size     : 100\n",
    "Hash table size        : 200000\n",
    "Temp directory         : ./\n",
    "Max open files         : 20\n",
    "FOF size               : 10\n",
    "n                      : 2\n",
    "Initialising hash table...\n",
    "Reading vocabulary...\n",
    "Allocating memory for the n-gram buffer...\n",
    "Reading text into the n-gram buffer...\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Sorting n-grams...\n",
    "Writing sorted n-grams to temporary file ./text2idngram.temp.hakim-vivobook-asuslaptop-x420ua.9966.1\n",
    "Merging temporary files...\n",
    "\n",
    "2-grams occurring:\tN times\t\t> N times\tSug. -spec_num value\n",
    "      0\t\t\t\t\t\t  16340\t\t  16513\n",
    "      1\t\t\t\t  13291\t\t   3049\t\t   3089\n",
    "      2\t\t\t\t   1732\t\t   1317\t\t   1340\n",
    "      3\t\t\t\t    542\t\t    775\t\t    792\n",
    "      4\t\t\t\t    268\t\t    507\t\t    522\n",
    "      5\t\t\t\t    143\t\t    364\t\t    377\n",
    "      6\t\t\t\t     91\t\t    273\t\t    285\n",
    "      7\t\t\t\t     52\t\t    221\t\t    233\n",
    "      8\t\t\t\t     48\t\t    173\t\t    184\n",
    "      9\t\t\t\t     34\t\t    139\t\t    150\n",
    "     10\t\t\t\t     20\t\t    119\t\t    130\n",
    "text2idngram : Done.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 7 : Reprendre les mêmes étapes (Q4, Q5 et Q6) mais générer les fichiers correspondants aux triplets de mots.\n",
    "Ajouter les cellules (text brut) nécessaires. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./CMU-Cam_Toolkit_v2/bin/text2wngram -n 3 -temp .  < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app3.wngram\n",
    "\n",
    "text2wngram\n",
    "n = 3\n",
    "Number of words in buffer = 9090909\n",
    "Number of chars in buffer = 63636363\n",
    "Max number of files open at once = 20\n",
    "Temporary directory = ./\n",
    "Allocated 63636363 bytes to text buffer.\n",
    "Allocated 72727272 bytes to pointer array.\n",
    "Reading text into buffer...\n",
    "Reading text into the n-gram buffer...\n",
    "20,000 words processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Sorting pointer array...\n",
    "Writing out temporary file ./text2wngram.tmp.pc65l.7561.1...\n",
    "Merging temporary files...\n",
    "Merging temp files 1 through 1...\n",
    "text2wngram : Done.\n",
    "\n",
    "Plus frequents :\n",
    "</S> <S> les 60\n",
    "</S> <S> le 59\n",
    "</S> <S> et 53\n",
    "</S> <S> la 52\n",
    "</S> <S> il 44\n",
    "</S> <S> c'est 42\n",
    "</S> <S> on 39\n",
    "</S> <S> l' 37\n",
    "</S> <S> bonjour 34\n",
    "</S> <S> en 29\n",
    "bonjour </S> <S> 26\n",
    "</S> <S> mais 25\n",
    "</S> <S> un 24\n",
    "</S> <S> alors 23\n",
    "</S> <S> <E0> 23\n",
    "<S> bonjour </S> 22\n",
    "</S> <S> merci 20\n",
    "Inter </S> <S> 20\n",
    "France Inter </S> 20\n",
    "</S> <S> une 19\n",
    "de Saddam Hussein 17\n",
    "</S> <S> pour 16\n",
    "Irak </S> <S> 16\n",
    "<S> il est 15\n",
    "\n",
    "Moins frequents :\n",
    "a aussit<F4>t ajout<E9> 1\n",
    "a aussi gagn<E9> 1\n",
    "a aussi fait 1\n",
    "a aurait tent<E9> 1\n",
    "a assum<E9> son 1\n",
    "a assum<E9> le 1\n",
    "<E7>a arrive je 1\n",
    "a a renforc<E9> 1\n",
    "a appris et 1\n",
    "<E7>a <E7>a n'a 1\n",
    "<E7>a a march<E9> 1\n",
    "<E7>a <E7>a m'a 1\n",
    "a affaire un 1\n",
    "<E7>a <E7>a fait 1\n",
    "<E7>a a du 1\n",
    "<E7>a a de 1\n",
    "<E7>a a d<FB> 1\n",
    "a <E0> accomplir 1\n",
    "<E7>a <E7>a <E7>a 1\n",
    "A86 entre Rosny 1\n",
    "A4 <E9>galement au 1\n",
    "A16 toujours coup<E9> 1\n",
    "A16 </S> <S> 1\n",
    "A13 sera interdite 1\n",
    "A11 pr<E8>s de 1\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/text2idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -temp . -n 3 -write_ascii < CORPUS_TP/CORPUS_APP/app.txt > CORPUS_TP/CORPUS_APP/app3.idngram\n",
    "\n",
    "text2idngram\n",
    "Vocab                  : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "N-gram buffer size     : 100\n",
    "Hash table size        : 200000\n",
    "Temp directory         : ./\n",
    "Max open files         : 20\n",
    "FOF size               : 10\n",
    "n                      : 3\n",
    "Initialising hash table...\n",
    "Reading vocabulary...\n",
    "Allocating memory for the n-gram buffer...\n",
    "Reading text into the n-gram buffer...\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Sorting n-grams...\n",
    "Writing sorted n-grams to temporary file ./text2idngram.temp.pc65l.7568.1\n",
    "Merging temporary files...\n",
    "\n",
    "2-grams occurring:\tN times\t\t> N times\tSug. -spec_num value\n",
    "      0\t\t\t\t\t\t  16340\t\t  16513\n",
    "      1\t\t\t\t  13291\t\t   3049\t\t   3089\n",
    "      2\t\t\t\t   1732\t\t   1317\t\t   1340\n",
    "      3\t\t\t\t    542\t\t    775\t\t    792\n",
    "      4\t\t\t\t    268\t\t    507\t\t    522\n",
    "      5\t\t\t\t    143\t\t    364\t\t    377\n",
    "      6\t\t\t\t     92\t\t    272\t\t    284\n",
    "      7\t\t\t\t     51\t\t    221\t\t    233\n",
    "      8\t\t\t\t     48\t\t    173\t\t    184\n",
    "      9\t\t\t\t     34\t\t    139\t\t    150\n",
    "     10\t\t\t\t     20\t\t    119\t\t    130\n",
    "\n",
    "3-grams occurring:\tN times\t\t> N times\tSug. -spec_num value\n",
    "      0\t\t\t\t\t\t  22119\t\t  22350\n",
    "      1\t\t\t\t  20247\t\t   1872\t\t   1900\n",
    "      2\t\t\t\t   1265\t\t    607\t\t    623\n",
    "      3\t\t\t\t    296\t\t    311\t\t    324\n",
    "      4\t\t\t\t    123\t\t    188\t\t    199\n",
    "      5\t\t\t\t     61\t\t    127\t\t    138\n",
    "      6\t\t\t\t     35\t\t     92\t\t    102\n",
    "      7\t\t\t\t     17\t\t     75\t\t     85\n",
    "      8\t\t\t\t     13\t\t     62\t\t     72\n",
    "      9\t\t\t\t     11\t\t     51\t\t     61\n",
    "     10\t\t\t\t      5\t\t     46\t\t     56\n",
    "text2idngram : Done.\n",
    "\n",
    "\n",
    "1 3428 3257 1\n",
    "2 1050 3689 1\n",
    "2 2758 2658 1\n",
    "2 3670 2051 2\n",
    "2 3670 4036 1\n",
    "2 3670 4624 1\n",
    "3 17 18 1\n",
    "3 2618 4474 1\n",
    "3 3990 4762 1\n",
    "3 4449 3993 1\n",
    "4 1932 3075 1\n",
    "4 4774 1356 1\n",
    "5 17 18 1\n",
    "5 2051 3349 1\n",
    "5 2652 992 1\n",
    "5 2683 4197 1\n",
    "5 3660 366 1\n",
    "5 3660 1112 1\n",
    "5 3732 2658 1\n",
    "5 4633 1908 1\n",
    "6 4633 1688 1\n",
    "6 4660 4036 1\n",
    "7 1962 2939 1\n",
    "7 2816 979 1\n",
    "8 17 18 1\n",
    "8 635 1264 1\n",
    "8 972 3599 1\n",
    "8 1975 2658 1\n",
    "8 2051 4633 1\n",
    "\n",
    "4797 1013 2750 1\n",
    "4797 1036 4303 1\n",
    "4797 1065 737 1\n",
    "4797 1101 4255 1\n",
    "4797 1630 2866 1\n",
    "4797 1818 17 1\n",
    "4797 1958 874 1\n",
    "4797 1962 4107 1\n",
    "4797 1999 2062 1\n",
    "4797 2128 17 1\n",
    "4797 2179 1630 1\n",
    "4797 2236 17 1\n",
    "4797 2243 17 1\n",
    "4797 2530 3196 1\n",
    "4797 3172 1962 1\n",
    "4797 3269 939 1\n",
    "4797 3574 3337 1\n",
    "4797 3589 2779 1\n",
    "4797 3780 4660 1\n",
    "4797 4129 2652 1\n",
    "4797 4416 4521 1\n",
    "4797 4475 4407 1\n",
    "4797 4478 17 1\n",
    "4797 4660 2652 1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 8 : A partir du fichier .idngram binaire généré précédemment, construire un modèle de langage bigramme pour chaque méthode de prélèvement proposée. Utiliser l’outil idngram2lm et l’option –arpa pour générer un fichier au format ARPA (.arpa). \n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# idngram2lm -idngram .idngram\n",
    "#           -vocab .vocab\n",
    "#           -arpa .arpa | -binary .binlm\n",
    "#         [ -context .ccs ]\n",
    "#         [ -calc_mem | -buffer 100 | -spec_num y ... z ]\n",
    "#         [ -vocab_type 1 ][ -oov_fraction 0.5 ]\n",
    "#         [ -linear | -absolute | -good_turing | -witten_bell ]\n",
    "#         [ -disc_ranges 1 7 7 ] [ -cutoffs 0 ... 0 ]\n",
    "#         [ -min_unicount 0 ][ -zeroton_fraction 1.0 ]\n",
    "#         [ -ascii_input | -bin_input ][ -n 3 ]  \n",
    "#         [ -verbosity 2 ] [ -four_byte_counts ]\n",
    "#         [ -two_byte_bo_weights\n",
    "#         [ -min_bo_weight -3.2 ] [ -max_bo_weight 2.5 ] \n",
    "#            [ -out_of_range_bo_weights 10000 ] ]\n",
    "\n",
    "Remarque : vous pouvez consulter la documentation section « Discounting strategies » pour voir les détails concernant les méthodes de prélèvement et calcul du discount ratio (delta ou d(r)). \n",
    "\n",
    "# exécuter la commande dans un terminal, copier coller la commande et la trace \n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app2.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app2linear.arpa -linear -n 2\n",
    "  n : 2\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app2.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app2linear.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Linear\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 25000000 2-grams.\n",
    "Allocated 100000000 bytes to table for 2-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    "\n",
    "Calculating discounted counts.\n",
    "Linear discounting ratios :\n",
    "1-gram : 0.893009\n",
    "2-gram : 0.489769\n",
    "Unigrams's discount mass is 0.106991 (n1/N = 0.106991)\n",
    "1 zerotons, P(zeroton) = 0.106991 P(singleton) = 3.42819e-05\n",
    "P(zeroton) was reduced to 0.0000342819 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.106956\n",
    "prob[UNK] = 3.83877e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 2-gram will be written to CORPUS_TP/CORPUS_APP/app2linear.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app2.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app2absolute.arpa -absolute -n 2\n",
    "  n : 2\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app2.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app2absolute.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Absolute\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 25000000 2-grams.\n",
    "Allocated 100000000 bytes to table for 2-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    "\n",
    "Calculating discounted counts.\n",
    "Absolute discounting ratios :\n",
    "1-gram : 0.363553 0.681777 0.787851 0.840888 0.872711  ... \n",
    "2-gram : 0.206744 0.603372 0.735581 0.801686 0.841349  ... \n",
    "Unigrams's discount mass is 0.117204 (n1/N = 0.106991)\n",
    "1 zerotons, P(zeroton) = 0.117204 P(singleton) = 1.39565e-05\n",
    "P(zeroton) was reduced to 0.0000139565 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.11719\n",
    "prob[UNK] = 1.58092e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 2-gram will be written to CORPUS_TP/CORPUS_APP/app2absolute.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app2.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app2goodturing.arpa -good_turing -n 2\n",
    "  n : 2\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app2.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app2goodturing.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Good-Turing\n",
    "     Discounting ranges :\n",
    "        1-gram : 1     2-gram : 7     \n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 25000000 2-grams.\n",
    "Allocated 100000000 bytes to table for 2-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    "\n",
    "Calculating discounted counts.\n",
    "Warning : 1-gram : Discounting range is 1; setting P(zeroton)=P(singleton).\n",
    "Discounted value : 1.00\n",
    "Warning : 2-gram : Some discount values are out of range;\n",
    "lowering discounting range to 6.\n",
    "Unigrams's discount mass is 3.83776e-05 (n1/N = 0.106991)\n",
    "1 zerotons, P(zeroton) = 3.83776e-05 P(singleton) = 3.83754e-05\n",
    "P(zeroton) was reduced to 0.0000383754 (1.000 of P(singleton))\n",
    "prob[UNK] = 3.83754e-05\n",
    "Incrementing contexts...\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 2-gram will be written to CORPUS_TP/CORPUS_APP/app2goodturing.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app2.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app2wittenbell.arpa -witten_bell -n 2\n",
    "  n : 2\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app2.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app2wittenbell.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Witten-Bell\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 25000000 2-grams.\n",
    "Allocated 100000000 bytes to table for 2-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    "\n",
    "Calculating discounted counts.\n",
    "Unigrams's discount mass is 0.155514 (n1/N = 0)\n",
    "1 zerotons, P(zeroton) = 0.155514 P(singleton) = 3.24191e-05\n",
    "P(zeroton) was reduced to 0.0000324191 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.155482\n",
    "prob[UNK] = 3.83877e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 2-gram will be written to CORPUS_TP/CORPUS_APP/app2wittenbell.arpa\n",
    "idngram2lm : Done.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Quelles informations sont données dans la trace. Quelles sont celles qui vous paraissent significatives. Comparer les valeurs obtenues lors de la génération des différents modèles.\n",
    "Commentez ici\n",
    "\n",
    "On a choisi un modèle ainsi qu'une méthode de prélèvement à chaque fois."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 9 : Reprendre l’étape précédente (Q8) pour générer un modèle de langage trigramme avec chacune des méthodes de prélèvement disponibles. \n",
    "\n",
    "Ajouter les cellules nécessaires pour garder la trace de votre travail\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app3.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app3linear.arpa -linear -n 3\n",
    "  n : 3\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app3.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app3linear.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     3-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Linear\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 5000000 2-grams.\n",
    "Allocated space for 12500000 3-grams.\n",
    "Allocated 50000000 bytes to table for 2-grams.\n",
    "Allocated 50000000 bytes to table for 3-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Calculating discounted counts.\n",
    "Linear discounting ratios :\n",
    "1-gram : 0.893005\n",
    "2-gram : 0.48975\n",
    "3-gram : 0.222704\n",
    "Unigrams's discount mass is 0.106995 (n1/N = 0.106995)\n",
    "1 zerotons, P(zeroton) = 0.106995 P(singleton) = 3.42831e-05\n",
    "P(zeroton) was reduced to 0.0000342831 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.10696\n",
    "prob[UNK] = 3.83892e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 3-gram will be written to CORPUS_TP/CORPUS_APP/app3linear.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app3.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app3absolute.arpa -absolute -n 3\n",
    "  n : 3\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app3.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app3absolute.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     3-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Absolute\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 5000000 2-grams.\n",
    "Allocated space for 12500000 3-grams.\n",
    "Allocated 50000000 bytes to table for 2-grams.\n",
    "Allocated 50000000 bytes to table for 3-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Calculating discounted counts.\n",
    "Absolute discounting ratios :\n",
    "1-gram : 0.363553 0.681777 0.787851 0.840888 0.872711  ... \n",
    "2-gram : 0.206744 0.603372 0.735581 0.801686 0.841349  ... \n",
    "3-gram : 0.111077 0.555538 0.703692 0.777769 0.822215  ... \n",
    "Unigrams's discount mass is 0.117208 (n1/N = 0.106995)\n",
    "1 zerotons, P(zeroton) = 0.117208 P(singleton) = 1.39571e-05\n",
    "P(zeroton) was reduced to 0.0000139571 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.117194\n",
    "prob[UNK] = 1.58099e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 3-gram will be written to CORPUS_TP/CORPUS_APP/app3absolute.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app3.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app3goodturing.arpa -good_turing -n 3\n",
    "  n : 3\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app3.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app3goodturing.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     3-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Good-Turing\n",
    "     Discounting ranges :\n",
    "        1-gram : 1     2-gram : 7     3-gram : 7     \n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 5000000 2-grams.\n",
    "Allocated space for 12500000 3-grams.\n",
    "Allocated 50000000 bytes to table for 2-grams.\n",
    "Allocated 50000000 bytes to table for 3-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Calculating discounted counts.\n",
    "Warning : 1-gram : Discounting range is 1; setting P(zeroton)=P(singleton).\n",
    "Discounted value : 1.00\n",
    "Warning : 2-gram : Some discount values are out of range;\n",
    "lowering discounting range to 6.\n",
    "Unigrams's discount mass is 3.83791e-05 (n1/N = 0.106995)\n",
    "1 zerotons, P(zeroton) = 3.83791e-05 P(singleton) = 3.83769e-05\n",
    "P(zeroton) was reduced to 0.0000383769 (1.000 of P(singleton))\n",
    "prob[UNK] = 3.83769e-05\n",
    "Incrementing contexts...\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 3-gram will be written to CORPUS_TP/CORPUS_APP/app3goodturing.arpa\n",
    "idngram2lm : Done.\n",
    "\n",
    "./CMU-Cam_Toolkit_v2/bin/idngram2lm -idngram CORPUS_TP/CORPUS_APP/app3.idngram -vocab CORPUS_TP/CORPUS_APP/app.vocab -arpa CORPUS_TP/CORPUS_APP/app3wittenbell.arpa -witten_bell -n 3\n",
    "  n : 3\n",
    "  Input file : CORPUS_TP/CORPUS_APP/app3.idngram     (binary format)\n",
    "  Output files :\n",
    "     ARPA format   : CORPUS_TP/CORPUS_APP/app3wittenbell.arpa\n",
    "  Vocabulary file : CORPUS_TP/CORPUS_APP/app.vocab\n",
    "  Cutoffs :\n",
    "     2-gram : 0     3-gram : 0     \n",
    "  Vocabulary type : Open - type 1\n",
    "  Minimum unigram count : 0\n",
    "  Zeroton fraction : 1\n",
    "  Counts will be stored in two bytes.\n",
    "  Count table size : 65535\n",
    "  Discounting method : Witten-Bell\n",
    "  Memory allocation for tree structure : \n",
    "     Allocate 100 MB of memory, shared equally between all n-gram tables.\n",
    "  Back-off weight storage : \n",
    "     Back-off weights will be stored in four bytes.\n",
    "Reading vocabulary.\n",
    "read_wlist_into_siht: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "read_wlist_into_array: a list of 4797 words was read from \"CORPUS_TP/CORPUS_APP/app.vocab\".\n",
    "Allocated space for 5000000 2-grams.\n",
    "Allocated space for 12500000 3-grams.\n",
    "Allocated 50000000 bytes to table for 2-grams.\n",
    "Allocated 50000000 bytes to table for 3-grams.\n",
    "Processing id n-gram file.\n",
    "20,000 n-grams processed for each \".\", 1,000,000 for each line.\n",
    ".\n",
    "Calculating discounted counts.\n",
    "Unigrams's discount mass is 0.15552 (n1/N = 0)\n",
    "1 zerotons, P(zeroton) = 0.15552 P(singleton) = 3.24202e-05\n",
    "P(zeroton) was reduced to 0.0000324202 (1.000 of P(singleton))\n",
    "Unigram was renormalized to absorb a mass of 0.155487\n",
    "prob[UNK] = 3.83892e-05\n",
    "Calculating back-off weights...\n",
    "Writing out language model...\n",
    "ARPA-style 3-gram will be written to CORPUS_TP/CORPUS_APP/app3wittenbell.arpa\n",
    "idngram2lm : Done.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Vous disposez normalement de 4 modèles de langage bigrammes et 4 modèles de langage trigrammes appris sur le même corpus. Vous allez les tester sur le même fichier de test. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EVALUTATION DES MODELES DE LANGAGE"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Calcul de la perplexité et du taux de mots hors vocabulaire. Utiliser le fichier qui se trouve dans CORPUS_TEST1 pour effectuer cette évaluation. \n",
    "\n",
    "QUESTION 10 : Evaluer chacun des 8 modèles générés précédemment en utilisant la combinaison de commandes suivante : \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# echo \"perplexity –text corpus_test -probs fichier.probs -oovs fichier.oovs -annotate fichier.annote \" | evallm –arpa # modele_a_tester\n",
    "\n",
    "#avec\n",
    "#\tevallm [ -binary .binlm | -arpa .arpa [ -context .ccs ] \n",
    "\n",
    "# exécuter les commandes d'évaluation dans un terminal, copier coller la commande et la trace\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/bilinear.probs -oovs CORPUS_TP/CORPUS_APP/bilinear.oovs -annotate CORPUS_TP/CORPUS_APP/bilinear.annote\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app2linear.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app2linear.arpa\n",
    "Reading in a 2-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/bilinear.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/bilinear.annote\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/bilinear.oovs\n",
    "Perplexity = 165.07, Entropy = 7.37 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 2-grams hit = 5657  (51.20%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/trilinear.probs -oovs CORPUS_TP/CORPUS_APP/trilinear.oovs -annotate CORPUS_TP/CORPUS_APP/trilinear.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app3linear.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app3linear.arpa\n",
    "Reading in a 3-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Number of 3-grams = 22119.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Reading 3-grams...\n",
    ".\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/trilinear.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/trilinear.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/trilinear.oovs\n",
    "Perplexity = 162.62, Entropy = 7.35 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 3-grams hit = 2482  (22.46%)\n",
    "Number of 2-grams hit = 3175  (28.74%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "evallm : \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/biabsolute.probs -oovs CORPUS_TP/CORPUS_APP/biabsolute.oovs -annotate CORPUS_TP/CORPUS_APP/biabsolute.annote\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app2absolute.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app2absolute.arpa\n",
    "Reading in a 2-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/biabsolute.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/biabsolute.annote\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/biabsolute.oovs\n",
    "Perplexity = 144.66, Entropy = 7.18 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 2-grams hit = 5657  (51.20%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/triabsolute.probs -oovs CORPUS_TP/CORPUS_APP/triabsolute.oovs -annotate CORPUS_TP/CORPUS_APP/triabsolute.annote\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app3absolute.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app3absolute.arpa\n",
    "Reading in a 3-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Number of 3-grams = 22119.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Reading 3-grams...\n",
    ".\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/triabsolute.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/triabsolute.annote\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/triabsolute.oovs\n",
    "Perplexity = 136.47, Entropy = 7.09 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 3-grams hit = 2482  (22.46%)\n",
    "Number of 2-grams hit = 3175  (28.74%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/bigoodturing.probs -oovs CORPUS_TP/CORPUS_APP/bigoodturing.oovs -annotate CORPUS_TP/CORPUS_APP/bigoodturing.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app2goodturing.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app2goodturing.arpa\n",
    "Reading in a 2-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/bigoodturing.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/bigoodturing.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/bigoodturing.oovs\n",
    "Perplexity = 143.26, Entropy = 7.16 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 2-grams hit = 5657  (51.20%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/trigoodturing.probs -oovs CORPUS_TP/CORPUS_APP/trigoodturtrng.oovs -annotate CORPUS_TP/CORPUS_APP/trigoodturtrng.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app3goodturing.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app3goodturing.arpa\n",
    "Reading in a 3-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Number of 3-grams = 22119.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Reading 3-grams...\n",
    ".\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/trigoodturing.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/trigoodturtrng.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/trigoodturtrng.oovs\n",
    "Perplexity = 136.28, Entropy = 7.09 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 3-grams hit = 2482  (22.46%)\n",
    "Number of 2-grams hit = 3175  (28.74%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/biwittenbell.probs -oovs CORPUS_TP/CORPUS_APP/biwittenbell.oovs -annotate CORPUS_TP/CORPUS_APP/biwittenbell.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app2wittenbell.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app2wittenbell.arpa\n",
    "Reading in a 2-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/biwittenbell.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/biwittenbell.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/biwittenbell.oovs\n",
    "Perplexity = 148.22, Entropy = 7.21 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 2-grams hit = 5657  (51.20%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1 -probs CORPUS_TP/CORPUS_APP/triwittenbell.probs -oovs CORPUS_TP/CORPUS_APP/triwittenbell.oovs -annotate CORPUS_TP/CORPUS_APP/triwittenbell.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app3wittenbell.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app3wittenbell.arpa\n",
    "Reading in a 3-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Number of 3-grams = 22119.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Reading 3-grams...\n",
    ".\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST1/20030416_0700_0800_FRANCEINTER_DGA.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/triwittenbell.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/triwittenbell.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/triwittenbell.oovs\n",
    "Perplexity = 138.99, Entropy = 7.12 bits\n",
    "Computation based on 11049 words.\n",
    "Number of 3-grams hit = 2482  (22.46%)\n",
    "Number of 2-grams hit = 3175  (28.74%)\n",
    "Number of 1-grams hit = 5392  (48.80%)\n",
    "1857 OOVs (14.39%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "evallm :\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Commenter une des traces\n",
    "La derniere trace donne\n",
    "Le nombre d'unigrammes, de bigrammes et de trigrammes observés, les probas des grammes dans le fichiers .probs, les annotations dans le fichier .annotate, le nombre de mots n'appartenant pas au vocabulaire (présents dans test mais pas dans vocab de l'apprentissage) et le taux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ANALYSE DES RESULTATS "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 11 : Présenter sous forme de tableau ou équivalent (cf. exemple) les différents résultats obtenus (OOV, Entropie, Perplexité) et déterminer quel est le modèle a priori le plus performant.\n",
    "\n",
    "\n",
    "Méthode de prélèvement\tEvaluation Modèle Bigramme PERPLEXITE et OOV\tEvaluation Modèle Trigramme PERPLEXITE et OOV\n",
    "\n",
    "- Linear\t            165,07;7,37                                             162,62;7,35\n",
    "\t\n",
    "- Absolute\t            144,66;7,18                                             136,47;7,09\n",
    "\t\n",
    "- Good Turing\t        143,26;7,16                                             136,28;7,09\n",
    "\t\n",
    "- Witten Bell\t        148,22;7,21                                             138,99;7,12\n",
    "\t\n",
    "OOV : 1857 (14.39%)\n",
    "\n",
    "Dans le tableau, (perplexité;entropie).\n",
    "\n",
    "Visiblement, le trigramme good turing est le plus performant (perplexité minimale, donc nombre de mots candidats minimal, et meilleure entropie, donc nombre de bits nécessaire pour coder l'info minimal)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 12 : Consulter les différents fichiers générés (.probs, .oovs, .annote). Copier-coller les 20 premières lignes (commande head) de chaque dans une cellule texte brut et commenter. (fait uniquement pour bilinear et trilinear)\n",
    "\n",
    "l'information de .probs est contenue dans .annote (c'est les probas des grammes générés), oovs est la liste des mots qui apparaissent dans test qui n'existent pas dans app).\n",
    "\n",
    "bilinear probs :\n",
    "0.049023\n",
    "0.00153426\n",
    "0.139927\n",
    "0.00976113\n",
    "0.00441266\n",
    "0.0259418\n",
    "0.489779\n",
    "0.000505708\n",
    "0.209894\n",
    "0.00392283\n",
    "0.0019059\n",
    "0.00928966\n",
    "0.00202395\n",
    "0.151985\n",
    "0.00326738\n",
    "0.0810588\n",
    "0.0631975\n",
    "0.489779\n",
    "3.61243e-05\n",
    "5.87625e-05\n",
    "0.000197106\n",
    "\n",
    "bilinear oovs\n",
    "commandes\n",
    "Paoli\n",
    "Stoufflet\n",
    "m'en\n",
    "partez\n",
    "conversation\n",
    "t<E9>l<E9>phonique\n",
    "brouille\n",
    "persister\n",
    "divergences\n",
    "d<E9>g<E2>ts\n",
    "palestinien\n",
    "Abou\n",
    "r<E9>volut(ion)\n",
    "r<E9>v<E9>lation\n",
    "financ<E9>\n",
    "caisses\n",
    "aval\n",
    "Giat\n",
    "Industries\n",
    "orgue\n",
    "mobilisation\n",
    "salari<E9>s\n",
    "social\n",
    "sites\n",
    "renoue\n",
    "glace\n",
    "discussions\n",
    "initiative\n",
    "\n",
    "\n",
    "bilinear annotate\n",
    "P( <S> | ) = 0.049023 logprob = -1.309600 bo_case = 1\n",
    "P( bonne | <S> ) = 0.00153426 logprob = -2.814100 bo_case = 2\n",
    "P( journ<E9>e | bonne ) = 0.139927 logprob = -0.854100 bo_case = 2\n",
    "P( <E0> | journ<E9>e ) = 0.00976113 logprob = -2.010500 bo_case = 2-1\n",
    "P( tous | <E0> ) = 0.00441266 logprob = -2.355300 bo_case = 2\n",
    "P( </S> | tous ) = 0.0259418 logprob = -1.586000 bo_case = 2-1\n",
    "P( <S> | </S> ) = 0.489779 logprob = -0.310000 bo_case = 2\n",
    "P( Fabrice | <S> ) = 0.000505708 logprob = -3.296100 bo_case = 2-1\n",
    "P( Drouelle | Fabrice ) = 0.209894 logprob = -0.678000 bo_case = 2\n",
    "P( est | Drouelle ) = 0.00392283 logprob = -2.406400 bo_case = 2-1\n",
    "P( aux | est ) = 0.0019059 logprob = -2.719900 bo_case = 2-1\n",
    "P( du | <UNK> ) = 0.00928966 logprob = -2.032000 bo_case = 2x1\n",
    "P( sept | du ) = 0.00202395 logprob = -2.693800 bo_case = 2\n",
    "P( neuf | sept ) = 0.151985 logprob = -0.818200 bo_case = 2\n",
    "P( ce | neuf ) = 0.00326738 logprob = -2.485800 bo_case = 2-1\n",
    "P( matin | ce ) = 0.0810588 logprob = -1.091200 bo_case = 2\n",
    "P( </S> | matin ) = 0.0631975 logprob = -1.199300 bo_case = 2\n",
    "P( <S> | </S> ) = 0.489779 logprob = -0.310000 bo_case = 2\n",
    "P( St<E9>phane | <S> ) = 3.61243e-05 logprob = -4.442200 bo_case = 2-1\n",
    "P( revient | St<E9>phane ) = 5.87625e-05 logprob = -4.230900 bo_case = 2-1\n",
    "P( demain | revient ) = 0.000197106 logprob = -3.705300 bo_case = 2-1\n",
    "P( </S> | demain ) = 0.0261216 logprob = -1.583000 bo_case = 2-1\n",
    "P( <S> | </S> ) = 0.489779 logprob = -0.310000 bo_case = 2\n",
    "P( oui | <S> ) = 0.00460257 logprob = -2.337000 bo_case = 2\n",
    "P( demain | oui ) = 0.000212814 logprob = -3.672000 bo_case = 2-1\n",
    "P( retour | demain ) = 0.000307114 logprob = -3.512700 bo_case = 2-1\n",
    "P( de | retour ) = 0.065298 logprob = -1.185100 bo_case = 2\n",
    "P( St<E9>phane | de ) = 2.72019e-05 logprob = -4.565400 bo_case = 2-1\n",
    "P( mon | <UNK> ) = 0.000230303 logprob = -3.637700 bo_case = 2x1\n",
    "\n",
    "\n",
    "trilinear probs \n",
    "\n",
    "0.049023\n",
    "0.00153391\n",
    "0.167032\n",
    "0.00943626\n",
    "0.00441266\n",
    "0.0277779\n",
    "0.489779\n",
    "0.000770371\n",
    "0.209894\n",
    "0.00597861\n",
    "0.00190634\n",
    "0.00928966\n",
    "0.00202395\n",
    "0.22269\n",
    "0.00330674\n",
    "0.0810588\n",
    "0.0278356\n",
    "0.22269\n",
    "5.503e-05\n",
    "5.87625e-05\n",
    "0.000197106\n",
    "0.0261276\n",
    "0.489779\n",
    "0.00209459\n",
    "0.0002509\n",
    "0.000307114\n",
    "\n",
    "\n",
    "trilinear oovs\n",
    "\n",
    "commandes\n",
    "Paoli\n",
    "Stoufflet\n",
    "m'en\n",
    "partez\n",
    "conversation\n",
    "t<E9>l<E9>phonique\n",
    "brouille\n",
    "persister\n",
    "divergences\n",
    "d<E9>g<E2>ts\n",
    "palestinien\n",
    "Abou\n",
    "r<E9>volut(ion)\n",
    "r<E9>v<E9>lation\n",
    "financ<E9>\n",
    "caisses\n",
    "aval\n",
    "Giat\n",
    "Industries\n",
    "orgue\n",
    "mobilisation\n",
    "salari<E9>s\n",
    "social\n",
    "sites\n",
    "renoue\n",
    "glace\n",
    "discussions\n",
    "initiative\n",
    "<C9>lys<E9>e\n",
    "agira\n",
    "pragmatique\n",
    "conversation\n",
    "positive\n",
    "\n",
    "\n",
    "trilinear annote\n",
    "\n",
    "P( <S> | ) = 0.049023 logprob = -1.309600 bo_case = 1\n",
    "P( bonne | <S> ) = 0.00153391 logprob = -2.814200 bo_case = 2\n",
    "P( journ<E9>e | <S> bonne ) = 0.167032 logprob = -0.777200 bo_case = 3\n",
    "P( <E0> | bonne journ<E9>e ) = 0.00943626 logprob = -2.025200 bo_case = 3-2-1\n",
    "P( tous | journ<E9>e <E0> ) = 0.00441266 logprob = -2.355300 bo_case = 3x2\n",
    "P( </S> | <E0> tous ) = 0.0277779 logprob = -1.556300 bo_case = 3-2-1\n",
    "P( <S> | tous </S> ) = 0.489779 logprob = -0.310000 bo_case = 3x2\n",
    "P( Fabrice | </S> <S> ) = 0.000770371 logprob = -3.113300 bo_case = 3-2-1\n",
    "P( Drouelle | <S> Fabrice ) = 0.209894 logprob = -0.678000 bo_case = 3x2\n",
    "P( est | Fabrice Drouelle ) = 0.00597861 logprob = -2.223400 bo_case = 3-2-1\n",
    "P( aux | Drouelle est ) = 0.00190634 logprob = -2.719800 bo_case = 3x2-1\n",
    "P( du | aux <UNK> ) = 0.00928966 logprob = -2.032000 bo_case = 3x2x1\n",
    "P( sept | <UNK> du ) = 0.00202395 logprob = -2.693800 bo_case = 3x2\n",
    "P( neuf | du sept ) = 0.22269 logprob = -0.652300 bo_case = 3\n",
    "P( ce | sept neuf ) = 0.00330674 logprob = -2.480600 bo_case = 3-2-1\n",
    "P( matin | neuf ce ) = 0.0810588 logprob = -1.091200 bo_case = 3x2\n",
    "P( </S> | ce matin ) = 0.0278356 logprob = -1.555400 bo_case = 3\n",
    "P( <S> | matin </S> ) = 0.22269 logprob = -0.652300 bo_case = 3\n",
    "P( St<E9>phane | </S> <S> ) = 5.503e-05 logprob = -4.259400 bo_case = 3-2-1\n",
    "P( revient | <S> St<E9>phane ) = 5.87625e-05 logprob = -4.230900 bo_case = 3x2-1\n",
    "P( demain | St<E9>phane revient ) = 0.000197106 logprob = -3.705300 bo_case = 3x2-1\n",
    "P( </S> | revient demain ) = 0.0261276 logprob = -1.582900 bo_case = 3x2-1\n",
    "P( <S> | demain </S> ) = 0.489779 logprob = -0.310000 bo_case = 3x2\n",
    "P( oui | </S> <S> ) = 0.00209459 logprob = -2.678900 bo_case = 3\n",
    "P( demain | <S> oui ) = 0.0002509 logprob = -3.600500 bo_case = 3-2-1\n",
    "P( retour | oui demain ) = 0.000307114 logprob = -3.512700 bo_case = 3x2-1\n",
    "P( de | demain retour ) = 0.065298 logprob = -1.185100 bo_case = 3x2\n",
    "P( St<E9>phane | retour de ) = 2.24802e-05 logprob = -4.648200 bo_case = 3-2-1\n",
    "P( mon | St<E9>phane <UNK> ) = 0.000230356 logprob = -3.637600 bo_case = 3x2x1\n",
    "P( cher | <UNK> mon ) = 7.84332e-05 logprob = -4.105500 bo_case = 3x2-1\n",
    "P( Nicolas | mon cher ) = 0.000121283 logprob = -3.916200 bo_case = 3x2-1\n",
    "P( </S> | Nicolas <UNK> ) = 0.0489892 logprob = -1.309900 bo_case = 3x2x1\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "QUESTION 13 : Utiliser votre meilleur modèle bigramme et votre meilleur modèle trigramme et évaluer les sur le fichier contenu dans CORPUS_TEST2. Copier-coller les résultats et commenter.   \n",
    "\n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST2/20030418_1700_1800_FRANCEINFO_DGA.dev.tlm1 -probs CORPUS_TP/CORPUS_APP/testbigoodturing.probs -oovs CORPUS_TP/CORPUS_APP/testbigoodturing.oovs -annotate CORPUS_TP/CORPUS_APP/testbigoodturing.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app2goodturing.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app2goodturing.arpa\n",
    "Reading in a 2-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST2/20030418_1700_1800_FRANCEINFO_DGA.dev.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/testbigoodturing.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/testbigoodturing.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/testbigoodturing.oovs\n",
    "Perplexity = 181.84, Entropy = 7.51 bits\n",
    "Computation based on 10130 words.\n",
    "Number of 2-grams hit = 4598  (45.39%)\n",
    "Number of 1-grams hit = 5532  (54.61%)\n",
    "2170 OOVs (17.64%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    " \n",
    "echo \"perplexity -text CORPUS_TP/CORPUS_TEST2/20030418_1700_1800_FRANCEINFO_DGA.dev.tlm1 -probs CORPUS_TP/CORPUS_APP/testtrigoodturing.probs -oovs CORPUS_TP/CORPUS_APP/testtrigoodturing.oovs -annotate CORPUS_TP/CORPUS_APP/testtrigoodturing.annotate\" | ./CMU-Cam_Toolkit_v2/bin/evallm -arpa CORPUS_TP/CORPUS_APP/app3goodturing.arpa \n",
    "Reading in language model from file CORPUS_TP/CORPUS_APP/app3goodturing.arpa\n",
    "Reading in a 3-gram language model.\n",
    "Number of 1-grams = 4798.\n",
    "Number of 2-grams = 16340.\n",
    "Number of 3-grams = 22119.\n",
    "Reading unigrams...\n",
    "\n",
    "Reading 2-grams...\n",
    "\n",
    "Reading 3-grams...\n",
    ".\n",
    "Done.\n",
    "evallm : Computing perplexity of the language model with respect\n",
    "   to the text CORPUS_TP/CORPUS_TEST2/20030418_1700_1800_FRANCEINFO_DGA.dev.tlm1\n",
    "Probability stream will be written to file CORPUS_TP/CORPUS_APP/testtrigoodturing.probs\n",
    "Annotation will be written to file CORPUS_TP/CORPUS_APP/testtrigoodturing.annotate\n",
    "Out of vocabulary words will be written to file CORPUS_TP/CORPUS_APP/testtrigoodturing.oovs\n",
    "Perplexity = 181.85, Entropy = 7.51 bits\n",
    "Computation based on 10130 words.\n",
    "Number of 3-grams hit = 1375  (13.57%)\n",
    "Number of 2-grams hit = 3222  (31.81%)\n",
    "Number of 1-grams hit = 5533  (54.62%)\n",
    "2170 OOVs (17.64%) and 0 context cues were removed from the calculation.\n",
    "evallm : Done.\n",
    "\n",
    "\n",
    "2170 mots sont hors vocabulaire, on voir le taux de bi et/ou trigrammes de la base d'app qui sont apparus dans le test, ainsi que la perplexité et l'entropie.\n",
    "Les résultats sont moins bons que lors du test avec le fichier précédent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faire un pdf de votre notebook (Ctrl P + imprimer dans un fichier) Déposer le notebook et sa version pdf sur moodle. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
